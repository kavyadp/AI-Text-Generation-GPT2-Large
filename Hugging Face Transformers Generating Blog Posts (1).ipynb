{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hackernoon.com/no-code-needs-to-adapt-to-specialists-an-argument-uo1w328p    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Using cached transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "Using cached huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Downloading safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.5 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.47.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--gpt2-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize Sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'No-Code Needs To Adapt to specialists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2949,    12, 10669, 36557,  1675, 30019,   284, 22447]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate and Decode Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "output = model.generate(input_ids, max_length=500, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2949,    12, 10669, 36557,  1675, 30019,   284, 22447,     6,  2476,\n",
       "           198,   198,   818,   262,  1613,  1178,   812,    11,   612,   468,\n",
       "           587,   257,  1256,   286,  1561,   546,   262,   761,   329,   257,\n",
       "           366,  3919,    12,  8189,     1,  3164,   284,  3788,  2478,    13,\n",
       "           383,  2126,   318,   326,   611,   345,   836,   470,   423,   284,\n",
       "          5490,   546, 19617,    11,   345,   460,  2962,   319,   584,  7612,\n",
       "           286,   262,  3788,    11,   884,   355,  1486,    11,  4856,    11,\n",
       "           290,  9262,    13,   198,    13,   764,   764,   290,   326,   338,\n",
       "           257,   922,  1517,    13,   887,   340,   338,   635,   257,  2089,\n",
       "          1517,    11,   780,   340,  1724,   326,   645,    12,   505,   481,\n",
       "           307,  1498,   284,  6068,   284,   262,  2476,   286, 22447,   287,\n",
       "           262,  2214,    13,   770,   318,  2592,  2081,   618,   340,  2058,\n",
       "           284,  1588,    12,  9888,  3788,  4493,    11,   810,   612,   389,\n",
       "           867,  1180, 22447,  1762,   319,   262,   976,  1917,    13,  1002,\n",
       "           345,   423,   257,  1917,   351,   530,   286,   777, 22447,    11,\n",
       "           340,   481,  1011,   257,   890,   640,   284,  3785,   503,   644,\n",
       "           262,  1917,   318,   290,   703,   284,  8494,   340,    13,   632,\n",
       "           481,   635,   307,   845,  2408,   329,   262, 22447,   284, 10996,\n",
       "           351,  1123,   584,    11,  1201,   484,   481,   477,   307,  1762,\n",
       "           287,   511,   898,  3313,   418,    13,  1081,   257,  1255,    11,\n",
       "           262,  2187,  1628,   481,  8659,   422,   257,  3092,   286, 19877,\n",
       "           290,  6946,    13,   554,  3090,    11,   428,  3164,   481,   787,\n",
       "           340,   881,   517,  2408,   284,  1064,   262,   826,  4610,   284,\n",
       "           257,  1948,  1917,    11,   543,   481,   287,  1210,  1085,   284,\n",
       "           772,   517,  2761,   866,   262,  2975,    13,  1114,  1672,    11,\n",
       "          1309,   338,   910,   326,   345,   761,   284,  1382,   257,  3992,\n",
       "          3586,   326,   481,  1249,  2985,   284,  2604,   287,   284,   511,\n",
       "          3331,  5504,    13,   921,  1244,   923,   416,  3597,   257,  2829,\n",
       "         17594,  1296,    11,   475,   788,   345,  1244,   765,   284,   751,\n",
       "           257,  1178,   517,  3033,   284,   787,   262, 17594,  1429,   517,\n",
       "          2836,    12, 13120,   290,  5713,    13,  3244,   345,  1183,   761,\n",
       "           257,  1074,   286,  6505,   284,   670,   319,   777,  3033,    13,\n",
       "           843,   788,    11,   618,   345,   821,  3492,   284,  2650,   262,\n",
       "          3586,    11,   534,  2985,   481,   423,   645,   835,   286,  4917,\n",
       "           503,   543,  3033,   423,   587,  9177,   290,   543,  3392,  4398,\n",
       "           470,    13,  1439,   286,   428,   318,   257,  3236,  7030,   286,\n",
       "           640,   290,  4133,    13,  1406,    11,   644,   815,   345,   466,\n",
       "          2427,    30,   383,   717,  1517,   345,   815,   466,   318,   787,\n",
       "          1654,   326,   477,   286,   534, 22447,   423,  1895,   284,   534,\n",
       "          2723,  2438,    11,   523,   326,   484,   460,   766,   703,   534,\n",
       "          2438,  2499,    13,  1320,   835,    11,   484,  1839,   470,   307,\n",
       "          7787,   284,  1265,   345,   329,  1037,   618,   484,   761,   340,\n",
       "            11,  2427,   286,  4953,   329,   345,   284,  1282,   510,   351,\n",
       "           257,  4610,   329,   606,    13,  6023,   922,   835,   284,  4155,\n",
       "           326,  2506,   468,   257,  2863,   284,  8676,   318,   284,   900,\n",
       "           510,   281,  1280,    12, 10459,  1628,   810,  2506,   460,  8676,\n",
       "           284,   290,  2987,   262,  2438,    13,   317,   922,  1672,   286,\n",
       "           884,   257,  1628,   318, 21722,    13,   785,    11,   257,  3052,\n",
       "           326,  3578,  2687,   284,  2251,   290,  2648,  1280,  2723,  4493,\n",
       "            13,  1318,   389,   257,  1271,   286,  1180,  3858,   286,  1280]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No-Code Needs To Adapt to specialists' needs\n",
      "\n",
      "In the past few years, there has been a lot of talk about the need for a \"no-code\" approach to software development. The idea is that if you don't have to worry about coding, you can focus on other aspects of the software, such as design, testing, and maintenance.\n",
      ". . . and that's a good thing. But it's also a bad thing, because it means that no-one will be able to adapt to the needs of specialists in the field. This is especially true when it comes to large-scale software projects, where there are many different specialists working on the same problem. If you have a problem with one of these specialists, it will take a long time to figure out what the problem is and how to solve it. It will also be very difficult for the specialists to communicate with each other, since they will all be working in their own silos. As a result, the whole project will suffer from a lack of coordination and communication. In addition, this approach will make it much more difficult to find the right solution to a particular problem, which will in turn lead to even more problems down the road. For example, let's say that you need to build a web application that will allow users to log in to their bank accounts. You might start by writing a simple login form, but then you might want to add a few more features to make the login process more user-friendly and secure. Then you'll need a team of developers to work on these features. And then, when you're ready to release the application, your users will have no way of finding out which features have been implemented and which ones haven't. All of this is a huge waste of time and resources. So, what should you do instead? The first thing you should do is make sure that all of your specialists have access to your source code, so that they can see how your code works. That way, they won't be afraid to ask you for help when they need it, instead of waiting for you to come up with a solution for them. Another good way to ensure that everyone has a chance to contribute is to set up an open-source project where everyone can contribute to and improve the code. A good example of such a project is GitHub.com, a website that allows anyone to create and share open source projects. There are a number of different types of open\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('specialist.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
